{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n!pip install keras \nfrom tensorflow.keras import models,layers\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.applications import *\nfrom tensorflow.keras.preprocessing import image","metadata":{"execution":{"iopub.status.busy":"2023-07-14T09:50:33.724228Z","iopub.execute_input":"2023-07-14T09:50:33.724840Z","iopub.status.idle":"2023-07-14T09:50:44.977272Z","shell.execute_reply.started":"2023-07-14T09:50:33.724783Z","shell.execute_reply":"2023-07-14T09:50:44.976037Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Requirement already satisfied: keras in /opt/conda/lib/python3.10/site-packages (2.12.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"batch = 32\nclasses = 'binary'\n\ntrain='../input/dataset/train'\ntest='../input/dataset/test'\nval='../input/dataset/val'\n","metadata":{"execution":{"iopub.status.busy":"2023-07-14T09:50:44.981354Z","iopub.execute_input":"2023-07-14T09:50:44.982111Z","iopub.status.idle":"2023-07-14T09:50:44.987744Z","shell.execute_reply.started":"2023-07-14T09:50:44.982074Z","shell.execute_reply":"2023-07-14T09:50:44.986691Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_datagen = image.ImageDataGenerator(\n    rotation_range=15,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    width_shift_range=0.1,\n    height_shift_range=0.1\n)\nvalidation_datagen= image.ImageDataGenerator()\n\ntest_datagen= image.ImageDataGenerator()\n\ntrain_generator = train_datagen.flow_from_directory(\n    train,\n    target_size = (224,224),\n    batch_size = batch,\n    class_mode = classes)\n\n\nvalidation_generator = validation_datagen.flow_from_directory(\n    val,\n    target_size = (224,224),\n    batch_size = batch,\n    shuffle=True,\n    class_mode = classes)\n\ntest_generator = test_datagen.flow_from_directory(\n    test,\n    target_size = (224,224),\n    batch_size = batch,\n    class_mode = classes)\nclass_names=validation_generator.class_indices\nclass_names\nlen(class_names)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T09:50:44.988870Z","iopub.execute_input":"2023-07-14T09:50:44.989502Z","iopub.status.idle":"2023-07-14T09:50:45.924674Z","shell.execute_reply.started":"2023-07-14T09:50:44.989468Z","shell.execute_reply":"2023-07-14T09:50:45.923549Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Found 4946 images belonging to 2 classes.\nFound 120 images belonging to 2 classes.\nFound 126 images belonging to 2 classes.\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}]},{"cell_type":"code","source":"base_for_model = tf.keras.applications.InceptionV3(weights='imagenet', input_shape=(224,224,3), include_top=False)\nfor layer in base_for_model.layers:\n    layer.trainable = False\n","metadata":{"execution":{"iopub.status.busy":"2023-07-14T09:50:45.926212Z","iopub.execute_input":"2023-07-14T09:50:45.926860Z","iopub.status.idle":"2023-07-14T09:50:48.367430Z","shell.execute_reply.started":"2023-07-14T09:50:45.926812Z","shell.execute_reply":"2023-07-14T09:50:48.366419Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(base_for_model) \nmodel.add(Dense(512, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(GlobalMaxPooling2D()) \nmodel.add(BatchNormalization())\nmodel.add(Dense(512, activation = 'relu')) \nmodel.add(BatchNormalization()) \nmodel.add(Dense(512,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-07-14T09:50:48.370710Z","iopub.execute_input":"2023-07-14T09:50:48.371489Z","iopub.status.idle":"2023-07-14T09:50:49.313918Z","shell.execute_reply.started":"2023-07-14T09:50:48.371454Z","shell.execute_reply":"2023-07-14T09:50:49.311959Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Model: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n inception_v3 (Functional)   (None, 5, 5, 2048)        21802784  \n                                                                 \n dense_5 (Dense)             (None, 5, 5, 512)         1049088   \n                                                                 \n batch_normalization_192 (Ba  (None, 5, 5, 512)        2048      \n tchNormalization)                                               \n                                                                 \n dense_6 (Dense)             (None, 5, 5, 512)         262656    \n                                                                 \n global_max_pooling2d_1 (Glo  (None, 512)              0         \n balMaxPooling2D)                                                \n                                                                 \n batch_normalization_193 (Ba  (None, 512)              2048      \n tchNormalization)                                               \n                                                                 \n dense_7 (Dense)             (None, 512)               262656    \n                                                                 \n batch_normalization_194 (Ba  (None, 512)              2048      \n tchNormalization)                                               \n                                                                 \n dense_8 (Dense)             (None, 512)               262656    \n                                                                 \n batch_normalization_195 (Ba  (None, 512)              2048      \n tchNormalization)                                               \n                                                                 \n dense_9 (Dense)             (None, 1)                 513       \n                                                                 \n=================================================================\nTotal params: 23,648,545\nTrainable params: 1,841,665\nNon-trainable params: 21,806,880\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"model.compile(\n    optimizer='adam',\n    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n    metrics=['accuracy','Precision','Recall','AUC']\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T09:50:49.315727Z","iopub.execute_input":"2023-07-14T09:50:49.316093Z","iopub.status.idle":"2023-07-14T09:50:49.342786Z","shell.execute_reply.started":"2023-07-14T09:50:49.316058Z","shell.execute_reply":"2023-07-14T09:50:49.341791Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"history=model.fit(\n    train_generator,\n    epochs=10,\n    batch_size=64,\n    validation_data=validation_generator\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T09:50:49.344373Z","iopub.execute_input":"2023-07-14T09:50:49.345051Z","iopub.status.idle":"2023-07-14T10:11:53.785268Z","shell.execute_reply.started":"2023-07-14T09:50:49.345017Z","shell.execute_reply":"2023-07-14T10:11:53.784115Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Epoch 1/10\n155/155 [==============================] - 124s 746ms/step - loss: 0.7238 - accuracy: 0.6488 - precision: 0.6531 - recall: 0.6575 - auc: 0.6974 - val_loss: 0.6254 - val_accuracy: 0.7333 - val_precision: 0.8947 - val_recall: 0.7391 - val_auc: 0.7642\nEpoch 2/10\n155/155 [==============================] - 114s 737ms/step - loss: 0.5794 - accuracy: 0.7010 - precision: 0.7046 - recall: 0.7077 - auc: 0.7750 - val_loss: 0.8564 - val_accuracy: 0.6167 - val_precision: 0.8194 - val_recall: 0.6413 - val_auc: 0.6215\nEpoch 3/10\n155/155 [==============================] - 114s 734ms/step - loss: 0.5375 - accuracy: 0.7291 - precision: 0.7310 - recall: 0.7380 - auc: 0.8069 - val_loss: 0.6158 - val_accuracy: 0.7667 - val_precision: 0.8265 - val_recall: 0.8804 - val_auc: 0.6925\nEpoch 4/10\n155/155 [==============================] - 114s 735ms/step - loss: 0.5218 - accuracy: 0.7351 - precision: 0.7395 - recall: 0.7384 - auc: 0.8203 - val_loss: 0.6431 - val_accuracy: 0.7167 - val_precision: 0.8021 - val_recall: 0.8370 - val_auc: 0.6411\nEpoch 5/10\n155/155 [==============================] - 118s 761ms/step - loss: 0.5153 - accuracy: 0.7442 - precision: 0.7510 - recall: 0.7423 - auc: 0.8226 - val_loss: 0.7776 - val_accuracy: 0.7167 - val_precision: 0.8085 - val_recall: 0.8261 - val_auc: 0.6782\nEpoch 6/10\n155/155 [==============================] - 114s 736ms/step - loss: 0.4941 - accuracy: 0.7560 - precision: 0.7614 - recall: 0.7563 - auc: 0.8397 - val_loss: 0.6345 - val_accuracy: 0.7417 - val_precision: 0.8352 - val_recall: 0.8261 - val_auc: 0.6980\nEpoch 7/10\n155/155 [==============================] - 114s 738ms/step - loss: 0.4905 - accuracy: 0.7570 - precision: 0.7609 - recall: 0.7603 - auc: 0.8419 - val_loss: 0.7473 - val_accuracy: 0.6500 - val_precision: 0.7778 - val_recall: 0.7609 - val_auc: 0.6295\nEpoch 8/10\n155/155 [==============================] - 114s 732ms/step - loss: 0.4724 - accuracy: 0.7667 - precision: 0.7694 - recall: 0.7718 - auc: 0.8541 - val_loss: 0.5884 - val_accuracy: 0.7667 - val_precision: 0.8721 - val_recall: 0.8152 - val_auc: 0.6918\nEpoch 9/10\n155/155 [==============================] - 117s 757ms/step - loss: 0.4760 - accuracy: 0.7693 - precision: 0.7714 - recall: 0.7754 - auc: 0.8540 - val_loss: 0.8385 - val_accuracy: 0.6667 - val_precision: 0.8421 - val_recall: 0.6957 - val_auc: 0.6378\nEpoch 10/10\n155/155 [==============================] - 115s 745ms/step - loss: 0.4656 - accuracy: 0.7758 - precision: 0.7751 - recall: 0.7865 - auc: 0.8599 - val_loss: 0.7425 - val_accuracy: 0.6500 - val_precision: 0.8378 - val_recall: 0.6739 - val_auc: 0.6308\n","output_type":"stream"}]},{"cell_type":"code","source":"score=model.evaluate(validation_generator)\nscore","metadata":{"execution":{"iopub.status.busy":"2023-07-14T10:11:53.787952Z","iopub.execute_input":"2023-07-14T10:11:53.789189Z","iopub.status.idle":"2023-07-14T10:12:01.402977Z","shell.execute_reply.started":"2023-07-14T10:11:53.789140Z","shell.execute_reply":"2023-07-14T10:12:01.401703Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"4/4 [==============================] - 6s 1s/step - loss: 0.7425 - accuracy: 0.6500 - precision: 0.8378 - recall: 0.6739 - auc: 0.6308\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"[0.7425212264060974,\n 0.6499999761581421,\n 0.837837815284729,\n 0.6739130616188049,\n 0.6308229565620422]"},"metadata":{}}]},{"cell_type":"markdown","source":"**Prediction of Oral Cancer using InceptionV3**\n\n\n**Introduction:**\n\nOral cancer is a significant global health concern, and early detection plays a vital role in improving patient outcomes. In this study, we propose a deep learning model based on the InceptionV3 architecture for predicting oral cancer. Our objective is to leverage the capabilities of deep learning and image analysis to develop an accurate and efficient tool for early oral cancer detection.\n\n**Methodology:**\n\n\n1) **Dataset:**\n\n* We utilized a comprehensive dataset of oral cancer images, consisting of two classes: \"cancer\" and \"non-cancer.\" The dataset was carefully curated and labeled by medical experts.\n\n\n2) **Data Preprocessing:**\n\n* To enhance the model's performance and mitigate overfitting, we applied various data preprocessing techniques. These included image augmentation techniques such as rotation, shear, zoom, horizontal flip, and brightness adjustments. Additionally, we divided the dataset into training, validation, and testing sets.\n\n\n3) **Model Architecture:**\n\n* We selected the InceptionV3 architecture as the base model due to its excellent performance in image classification tasks. InceptionV3 incorporates the concept of inception modules, enabling the network to efficiently learn spatial hierarchies from images. We fine-tuned the architecture specifically for oral cancer detection by adding extra layers.\n\n\n4) **Training and Evaluation:**\n\n* The model was trained using the training set and evaluated on the validation set. We employed the Adam optimizer with a binary cross-entropy loss function. Throughout training, we monitored evaluation metrics such as accuracy, precision, recall, and area under the curve (AUC) to assess the model's performance.\n\n\n5) **Results:**\n\n* The trained InceptionV3 model demonstrated promising results in predicting oral cancer. We achieved an accuracy of 0.6499999761581421, precision of 0.837837815284729 , recall of 0.6739130616188049, and AUC of 0.6308229565620422. These results indicate the model's capability to accurately classify oral cancer cases.\n\n**Conclusion:**\nIn this study, we developed a deep learning model based on the InceptionV3 architecture for predicting oral cancer. By leveraging InceptionV3's powerful image analysis capabilities, our model effectively extracts features from oral cancer images. This research contributes to the field of medical imaging and provides a potential tool for early detection of oral cancer. Further advancements and research can significantly impact the diagnosis and treatment of oral cancer.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}